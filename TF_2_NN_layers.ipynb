{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_2_NN_layers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyDearGreatTeacher/AI4high/blob/master/TF_2_NN_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdejCZY-VO4z",
        "colab_type": "text"
      },
      "source": [
        "# 神經網路常用的layers實作\n",
        "\n",
        "\n",
        "TensorFlow機器學習實戰指南 (美)尼克‧麥克盧爾 機械工業\n",
        "\n",
        "https://github.com/nfmcclure/tensorflow_cookbook\n",
        "\n",
        "6.5 用TensorFlow實現神經網路常見層 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEgoDtnPVJzy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "7c4e2a9e-78a7-4b13-fdf4-ed02ec3d6ea4"
      },
      "source": [
        "\"\"\"\n",
        "Implementing Different Layers\n",
        "We will illustrate how to use different types of layers in TensorFlow\n",
        "The layers of interest are:\n",
        " (1) Convolutional Layer\n",
        " (2) Activation Layer\n",
        " (3) Max-Pool Layer\n",
        " (4) Fully Connected Layer\n",
        " \n",
        "We will generate two different data sets for this\n",
        " script, a 1-D data set (row of data) and\n",
        " a 2-D data set (similar to picture)\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "\n",
        "# ---------------------------------------------------|\n",
        "# -------------------1D-data-------------------------|\n",
        "# ---------------------------------------------------|\n",
        "\n",
        "# Create graph session\n",
        "sess = tf.Session()\n",
        "\n",
        "# parameters for the run\n",
        "data_size = 25\n",
        "conv_size = 5\n",
        "maxpool_size = 5\n",
        "stride_size = 1\n",
        "\n",
        "# ensure reproducibility\n",
        "seed = 13\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Generate 1D data\n",
        "data_1d = np.random.normal(size=data_size)\n",
        "\n",
        "# Placeholder\n",
        "x_input_1d = tf.placeholder(dtype=tf.float32, shape=[data_size])\n",
        "\n",
        "\n",
        "# --------Convolution--------\n",
        "def conv_layer_1d(input_1d, input_filter, stride):\n",
        "    \"\"\"\n",
        "    TensorFlow's 'conv2d()' function only works with 4D arrays:\n",
        "    [batch#, width, height, channels], we have 1 batch, and\n",
        "    width = 1, but height = the length of the input, and 1 channel.\n",
        "    So next we create the 4D array by inserting dimension 1's.\n",
        "    :param input_1d: 1D input array.\n",
        "    :param input_filter: Filter to convolve across the input_1d array.\n",
        "    :param stride: stride for filter.\n",
        "    :return: array.\n",
        "    \"\"\"\n",
        "    input_2d = tf.expand_dims(input_1d, 0)\n",
        "    input_3d = tf.expand_dims(input_2d, 0)\n",
        "    input_4d = tf.expand_dims(input_3d, 3)\n",
        "    # Perform convolution with stride = 1, if we wanted to increase the stride,\n",
        "    # to say '2', then strides=[1,1,2,1]\n",
        "    convolution_output = tf.nn.conv2d(input_4d,\n",
        "                                      filter=input_filter,\n",
        "                                      strides=[1, 1, stride, 1],\n",
        "                                      padding=\"VALID\")\n",
        "    # Get rid of extra dimensions\n",
        "    conv_output_1d = tf.squeeze(convolution_output)\n",
        "    return conv_output_1d\n",
        "\n",
        "# Create filter for convolution.\n",
        "my_filter = tf.Variable(tf.random_normal(shape=[1, conv_size, 1, 1]))\n",
        "# Create convolution layer\n",
        "my_convolution_output = conv_layer_1d(x_input_1d, my_filter, stride=stride_size)\n",
        "\n",
        "\n",
        "# --------Activation--------\n",
        "def activation(input_1d):\n",
        "    return tf.nn.relu(input_1d)\n",
        "\n",
        "# Create activation layer\n",
        "my_activation_output = activation(my_convolution_output)\n",
        "\n",
        "\n",
        "# --------Max Pool--------\n",
        "def max_pool(input_1d, width, stride):\n",
        "    \"\"\"\n",
        "    Just like 'conv2d()' above, max_pool() works with 4D arrays.\n",
        "    [batch_size=1, width=1, height=num_input, channels=1]\n",
        "    :param input_1d: Input array to perform max-pool on.\n",
        "    :param width: Width of 1d-window for max-pool\n",
        "    :param stride: Stride of window across input array\n",
        "    :return: max-pooled array\n",
        "    \"\"\"\n",
        "    input_2d = tf.expand_dims(input_1d, 0)\n",
        "    input_3d = tf.expand_dims(input_2d, 0)\n",
        "    input_4d = tf.expand_dims(input_3d, 3)\n",
        "    # Perform the max pooling with strides = [1,1,1,1]\n",
        "    # If we wanted to increase the stride on our data dimension, say by\n",
        "    # a factor of '2', we put strides = [1, 1, 2, 1]\n",
        "    # We will also need to specify the width of the max-window ('width')\n",
        "    pool_output = tf.nn.max_pool(input_4d, ksize=[1, 1, width, 1],\n",
        "                                 strides=[1, 1, stride, 1],\n",
        "                                 padding='VALID')\n",
        "    # Get rid of extra dimensions\n",
        "    pool_output_1d = tf.squeeze(pool_output)\n",
        "    return pool_output_1d\n",
        "\n",
        "my_maxpool_output = max_pool(my_activation_output, width=maxpool_size, stride=stride_size)\n",
        "\n",
        "\n",
        "# --------Fully Connected--------\n",
        "def fully_connected(input_layer, num_outputs):\n",
        "    # First we find the needed shape of the multiplication weight matrix:\n",
        "    # The dimension will be (length of input) by (num_outputs)\n",
        "    weight_shape = tf.squeeze(tf.stack([tf.shape(input_layer), [num_outputs]]))\n",
        "    # Initialize such weight\n",
        "    weight = tf.random_normal(weight_shape, stddev=0.1)\n",
        "    # Initialize the bias\n",
        "    bias = tf.random_normal(shape=[num_outputs])\n",
        "    # Make the 1D input array into a 2D array for matrix multiplication\n",
        "    input_layer_2d = tf.expand_dims(input_layer, 0)\n",
        "    # Perform the matrix multiplication and add the bias\n",
        "    full_output = tf.add(tf.matmul(input_layer_2d, weight), bias)\n",
        "    # Get rid of extra dimensions\n",
        "    full_output_1d = tf.squeeze(full_output)\n",
        "    return full_output_1d\n",
        "\n",
        "my_full_output = fully_connected(my_maxpool_output, 5)\n",
        "\n",
        "# Initialize Variables\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "feed_dict = {x_input_1d: data_1d}\n",
        "\n",
        "print('>>>> 1D Data <<<<')\n",
        "\n",
        "# Convolution Output\n",
        "print('Input = array of length {}'.format(x_input_1d.shape.as_list()[0]))\n",
        "print('Convolution w/ filter, length = {}, stride size = {},'\n",
        "      'results in an array of length {}:'.format(conv_size,\n",
        "                                                 stride_size,\n",
        "                                                 my_convolution_output.shape.as_list()[0]))\n",
        "print(sess.run(my_convolution_output, feed_dict=feed_dict))\n",
        "\n",
        "# Activation Output\n",
        "print('\\nInput = above array of length {}'.format(my_convolution_output.shape.as_list()[0]))\n",
        "print('ReLU element wise returns '\n",
        "      'an array of length {}:'.format(my_activation_output.shape.as_list()[0]))\n",
        "print(sess.run(my_activation_output, feed_dict=feed_dict))\n",
        "\n",
        "# Max Pool Output\n",
        "print('\\nInput = above array of length {}'.format(my_activation_output.shape.as_list()[0]))\n",
        "print('MaxPool, window length = {}, stride size = {},'\n",
        "      'results in the array of length {}'.format(maxpool_size,\n",
        "                                                 stride_size,\n",
        "                                                 my_maxpool_output.shape.as_list()[0]))\n",
        "print(sess.run(my_maxpool_output, feed_dict=feed_dict))\n",
        "\n",
        "# Fully Connected Output\n",
        "print('\\nInput = above array of length {}'.format(my_maxpool_output.shape.as_list()[0]))\n",
        "print('Fully connected layer on all 4 rows '\n",
        "      'with {} outputs:'.format(my_full_output.shape.as_list()[0]))\n",
        "print(sess.run(my_full_output, feed_dict=feed_dict))\n",
        "\n",
        "# ---------------------------------------------------|\n",
        "# -------------------2D-data-------------------------|\n",
        "# ---------------------------------------------------|\n",
        "\n",
        "# Reset Graph\n",
        "ops.reset_default_graph()\n",
        "sess = tf.Session()\n",
        "\n",
        "# Parameters for the run\n",
        "row_size = 10\n",
        "col_size = 10\n",
        "conv_size = 2\n",
        "conv_stride_size = 2\n",
        "maxpool_size = 2\n",
        "maxpool_stride_size = 1\n",
        "\n",
        "# Set seed to ensure reproducibility\n",
        "seed = 13\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# Generate 2D data\n",
        "data_size = [row_size, col_size]\n",
        "data_2d = np.random.normal(size=data_size)\n",
        "\n",
        "# --------Placeholder--------\n",
        "x_input_2d = tf.placeholder(dtype=tf.float32, shape=data_size)\n",
        "\n",
        "\n",
        "# Convolution\n",
        "def conv_layer_2d(input_2d, conv_filter, conv_stride):\n",
        "    \"\"\"\n",
        "    TensorFlow's 'conv2d()' function only works with 4D arrays:\n",
        "    [batch#, width, height, channels], we have 1 batch, and\n",
        "    1 channel, but we do have width AND height this time.\n",
        "    So next we create the 4D array by inserting dimension 1's.\n",
        "    :param input_2d: input array for 2D convolution.\n",
        "    :param conv_filter: 2D-filter.\n",
        "    :param conv_stride: 2D stride settings.\n",
        "    :return: Convoluted array.\n",
        "    \"\"\"\n",
        "    input_3d = tf.expand_dims(input_2d, 0)\n",
        "    input_4d = tf.expand_dims(input_3d, 3)\n",
        "    # Note the stride difference below!\n",
        "    convolution_output = tf.nn.conv2d(input_4d,\n",
        "                                      filter=conv_filter,\n",
        "                                      strides=[1, conv_stride, conv_stride, 1],\n",
        "                                      padding=\"VALID\")\n",
        "    # Get rid of unnecessary dimensions\n",
        "    conv_output_2d = tf.squeeze(convolution_output)\n",
        "    return conv_output_2d\n",
        "\n",
        "# Create Convolutional Filter\n",
        "my_filter = tf.Variable(tf.random_normal(shape=[conv_size, conv_size, 1, 1]))\n",
        "# Create Convolutional Layer\n",
        "my_convolution_output = conv_layer_2d(x_input_2d, my_filter, conv_stride=conv_stride_size)\n",
        "\n",
        "\n",
        "# --------Activation--------\n",
        "def activation(input_1d):\n",
        "    return tf.nn.relu(input_1d)\n",
        "\n",
        "# Create Activation Layer\n",
        "my_activation_output = activation(my_convolution_output)\n",
        "\n",
        "\n",
        "# --------Max Pool--------\n",
        "def max_pool(input_2d, width, height, stride):\n",
        "    \"\"\"\n",
        "    Just like 'conv2d()' above, max_pool() works with 4D arrays.\n",
        "    [batch_size=1, width=given, height=given, channels=1]\n",
        "    :param input_2d: 2D input array\n",
        "    :param width: width of 2D max pool window\n",
        "    :param height: height of 2D max pool window\n",
        "    :param stride: 2d stride setting\n",
        "    :return: max-pool'ed array\n",
        "    \"\"\"\n",
        "    input_3d = tf.expand_dims(input_2d, 0)\n",
        "    input_4d = tf.expand_dims(input_3d, 3)\n",
        "    # Perform the max pooling with strides = [1,1,1,1]\n",
        "    # If we wanted to increase the stride on our data dimension, say by\n",
        "    # a factor of '2', we put strides = [1, 2, 2, 1]\n",
        "    pool_output = tf.nn.max_pool(input_4d, ksize=[1, height, width, 1],\n",
        "                                 strides=[1, stride, stride, 1],\n",
        "                                 padding='VALID')\n",
        "    # Get rid of unnecessary dimensions\n",
        "    pool_output_2d = tf.squeeze(pool_output)\n",
        "    return pool_output_2d\n",
        "\n",
        "# Create Max-Pool Layer\n",
        "my_maxpool_output = max_pool(my_activation_output, \n",
        "                             width=maxpool_size,\n",
        "                             height=maxpool_size,\n",
        "                             stride=maxpool_stride_size)\n",
        "\n",
        "\n",
        "# -------Fully Connected--------\n",
        "def fully_connected(input_layer, num_outputs):\n",
        "    \"\"\"\n",
        "    In order to connect our whole W byH 2d array, we first flatten it out to\n",
        "    a W times H 1D array.\n",
        "    :param input_layer: input array for fully connected layer.\n",
        "    :param num_outputs: how many outputs to give from layer.\n",
        "    :return: array of size num_outputs\n",
        "    \"\"\"\n",
        "    flat_input = tf.reshape(input_layer, [-1])\n",
        "    # We then find out how long it is, and create an array for the shape of\n",
        "    # the multiplication weight = (WxH) by (num_outputs)\n",
        "    weight_shape = tf.squeeze(tf.stack([tf.shape(flat_input), [num_outputs]]))\n",
        "    # Initialize the weight\n",
        "    weight = tf.random_normal(weight_shape, stddev=0.1)\n",
        "    # Initialize the bias\n",
        "    bias = tf.random_normal(shape=[num_outputs])\n",
        "    # Now make the flat 1D array into a 2D array for multiplication\n",
        "    input_2d = tf.expand_dims(flat_input, 0)\n",
        "    # Multiply and add the bias\n",
        "    full_output = tf.add(tf.matmul(input_2d, weight), bias)\n",
        "    # Get rid of extra dimension\n",
        "    full_output_2d = tf.squeeze(full_output)\n",
        "    return full_output_2d\n",
        "\n",
        "# Create Fully Connected Layer\n",
        "my_full_output = fully_connected(my_maxpool_output, 5)\n",
        "\n",
        "# Run graph\n",
        "# Initialize Variables\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "feed_dict = {x_input_2d: data_2d}\n",
        "\n",
        "print('\\n>>>> 2D Data <<<<')\n",
        "\n",
        "# Convolution Output\n",
        "print('Input = {} array'.format(x_input_2d.shape.as_list()))\n",
        "print('{} Convolution, stride size = [{}, {}], '\n",
        "      'results in the {} array'.format(my_filter.get_shape().as_list()[:2],\n",
        "                                       conv_stride_size,\n",
        "                                       conv_stride_size,\n",
        "                                       my_convolution_output.shape.as_list()))\n",
        "print(sess.run(my_convolution_output, feed_dict=feed_dict))\n",
        "\n",
        "# Activation Output\n",
        "print('\\nInput = the above {} array'.format(my_convolution_output.shape.as_list()))\n",
        "print('ReLU element wise returns the {} array'.format(my_activation_output.shape.as_list()))\n",
        "print(sess.run(my_activation_output, feed_dict=feed_dict))\n",
        "\n",
        "# Max Pool Output\n",
        "print('\\nInput = the above {} array'.format(my_activation_output.shape.as_list()))\n",
        "print('MaxPool, stride size = [{}, {}], '\n",
        "      'results in {} array'.format(maxpool_stride_size,\n",
        "                                   maxpool_stride_size,\n",
        "                                   my_maxpool_output.shape.as_list()))\n",
        "print(sess.run(my_maxpool_output, feed_dict=feed_dict))\n",
        "\n",
        "# Fully Connected Output\n",
        "print('\\nInput = the above {} array'.format(my_maxpool_output.shape.as_list()))\n",
        "print('Fully connected layer on all {} rows '\n",
        "      'results in {} outputs:'.format(my_maxpool_output.shape.as_list()[0],\n",
        "                                      my_full_output.shape.as_list()[0]))\n",
        "print(sess.run(my_full_output, feed_dict=feed_dict))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            ">>>> 1D Data <<<<\n",
            "Input = array of length 25\n",
            "Convolution w/ filter, length = 5, stride size = 1,results in an array of length 21:\n",
            "[-2.6357634 -1.1155049 -0.9557142 -1.696703  -0.3569939  0.6226649\n",
            "  4.4331603  2.013649   1.3304467 -2.3062966 -0.8291624 -2.6359417\n",
            "  0.7666935 -2.4646509 -2.285504   1.4978067  1.6960565  1.4855739\n",
            " -2.7979949  1.1814919  1.4214658]\n",
            "\n",
            "Input = above array of length 21\n",
            "ReLU element wise returns an array of length 21:\n",
            "[0.        0.        0.        0.        0.        0.6226649 4.4331603\n",
            " 2.013649  1.3304467 0.        0.        0.        0.7666935 0.\n",
            " 0.        1.4978067 1.6960565 1.4855739 0.        1.1814919 1.4214658]\n",
            "\n",
            "Input = above array of length 21\n",
            "MaxPool, window length = 5, stride size = 1,results in the array of length 17\n",
            "[0.        0.6226649 4.4331603 4.4331603 4.4331603 4.4331603 4.4331603\n",
            " 2.013649  1.3304467 0.7666935 0.7666935 1.4978067 1.6960565 1.6960565\n",
            " 1.6960565 1.6960565 1.6960565]\n",
            "\n",
            "Input = above array of length 17\n",
            "Fully connected layer on all 4 rows with 5 outputs:\n",
            "[ 1.7153609  -0.7234098  -1.224851   -2.5412786  -0.16338283]\n",
            "\n",
            ">>>> 2D Data <<<<\n",
            "Input = [10, 10] array\n",
            "[2, 2] Convolution, stride size = [2, 2], results in the [5, 5] array\n",
            "[[ 0.14431179  0.7278337   1.5114917  -1.2809976   1.7843919 ]\n",
            " [-2.5450304   0.76156765 -0.51650006  0.77131087  0.37542337]\n",
            " [ 0.4934591   0.01592219  0.38653135 -1.4799765   0.69527644]\n",
            " [-0.3461718  -2.5318975  -0.9525758  -1.4357066   0.6625736 ]\n",
            " [-1.9854025   0.34398782  2.5376048  -0.8678482  -0.3100495 ]]\n",
            "\n",
            "Input = the above [5, 5] array\n",
            "ReLU element wise returns the [5, 5] array\n",
            "[[0.14431179 0.7278337  1.5114917  0.         1.7843919 ]\n",
            " [0.         0.76156765 0.         0.77131087 0.37542337]\n",
            " [0.4934591  0.01592219 0.38653135 0.         0.69527644]\n",
            " [0.         0.         0.         0.         0.6625736 ]\n",
            " [0.         0.34398782 2.5376048  0.         0.        ]]\n",
            "\n",
            "Input = the above [5, 5] array\n",
            "MaxPool, stride size = [1, 1], results in [4, 4] array\n",
            "[[0.76156765 1.5114917  1.5114917  1.7843919 ]\n",
            " [0.76156765 0.76156765 0.77131087 0.77131087]\n",
            " [0.4934591  0.38653135 0.38653135 0.69527644]\n",
            " [0.34398782 2.5376048  2.5376048  0.6625736 ]]\n",
            "\n",
            "Input = the above [4, 4] array\n",
            "Fully connected layer on all 4 rows results in 5 outputs:\n",
            "[ 0.08245841 -0.16351229 -0.55429065 -0.24322602 -0.99900764]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}